{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garylau1/vertex-minors/blob/master/Complete_script_customerData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b864oO6hpTqX"
      },
      "source": [
        "\n",
        "## In this task we would do a vision classification task in Brain tumor from scatch.\n",
        "\n",
        "### Our result:\n",
        "The accuracy is around 75 in testing set after trying different layers.\n",
        "This model is very well in detecting class 1,2 and 3 (meningioma_tumor,no_tumor nd pituitary_tumour) but fail to detect class 0 (glimoa_tumor).\n",
        "\n",
        "### The most significance is:\n",
        "\n",
        "- We build up our linear layers ,relu layers and loss functions from scatch from basic class objects(e.g writing the whole formula inside the softmax function and cross-entropy loss)\n",
        "\n",
        "\n",
        "- We build up out custom dataset from scatch as well.\n",
        "\n",
        "\n",
        "- We write into python scripts and run it directly using command line in colab.\n",
        "\n",
        "### Future work\n",
        "\n",
        "- We might try transfer learning\n",
        "- We can try different paraemeters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile path_loader.py\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "out_path=Path(\"out_model_set\")\n",
        "os.makedirs(out_path,exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "Imagepath=(\"/content/Brain-Tumor-Classification-DataSet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrR8mBbuLLMV",
        "outputId": "5b9cc66b-e6b4-4c62-eeae-36927cea617e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing path_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python path_loader.py"
      ],
      "metadata": {
        "id": "xvEXqUnRr9UM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile out_model_set/data_creator.py\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_classes(dict1):\n",
        "  \"\"\"find the class fikder bane\"\"\"\n",
        "\n",
        "  found=sorted([entry1.name for entry1 in list(os.scandir(dict1))if entry1.is_dir()])\n",
        "  if not found:\n",
        "      raise  FileNotFoundError(f\"no such class1111 in {dict1}\")\n",
        "\n",
        "  class_index={class_name: i for i,class_name in enumerate(found)}\n",
        "  return found,class_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class customdataset(Dataset):\n",
        "\n",
        "  def find_classes(self,dict1):\n",
        "\n",
        "    found=sorted([entry1.name for entry1 in list(os.scandir(dict1))if entry1.is_dir()])\n",
        "    #found=[entry1.name for entry1 in list(os.scandir(dict1))if entry1.is_dir()]\n",
        "    if not found:\n",
        "      raise  FileNotFoundError(f\"no such class1111 in {dict1}\")\n",
        "\n",
        "    class_index={class_name: i for i,class_name in enumerate(found)}\n",
        "    return found,class_index\n",
        "\n",
        "\n",
        "  def __init__(self,target_folder,transform=None):\n",
        "    #self.targetpath=list(target_folder.glob(\"*/*.jpg\"))\n",
        "    self.targetpath=sorted(list(pathlib.Path(target_folder).glob(\"*/*.jpg\")))\n",
        "    self.transform=transform\n",
        "    self.classes,self.class_to_idx=self.find_classes(target_folder)\n",
        "\n",
        "    self.transform_2=transforms.Compose(\n",
        "    [transforms.Resize((224,224)),transforms.RandomHorizontalFlip(p=0.5),transforms.ToTensor()])\n",
        "\n",
        "\n",
        "  def load_image(self,index):\n",
        "      return (Image.open(self.targetpath[index]))\n",
        "  def __len__(self):\n",
        "      a=self.targetpath\n",
        "      return len(a)\n",
        "\n",
        "  def __getitem__(self,inx):\n",
        "\n",
        "    load=(self.load_image(inx))\n",
        "    name=self.targetpath[inx].parent.name\n",
        "    class_index=self.class_to_idx[name]\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(load),class_index\n",
        "    else:\n",
        "      return self.transform_2(load),class_index\n",
        "\n",
        "\n",
        "data_transform_aug=transforms.Compose([transforms.Resize((228,228)),transforms.TrivialAugmentWide(num_magnitude_bins=31),transforms.ToTensor()])\n",
        "\n",
        "data_transform_simple=transforms.Compose(\n",
        "    [transforms.Resize((228,228)),transforms.ToTensor()])\n",
        "\n",
        "def customdataset_loader(batchs,train_dir,test_dir,transform=data_transform_aug,transform2=data_transform_simple):\n",
        "  train_dataset=customdataset(train_dir,data_transform_aug)\n",
        "  test_dataset=customdataset(test_dir,data_transform_simple)\n",
        "\n",
        "  class_num=len(train_dataset.classes)\n",
        "\n",
        "  train_loader_simple=DataLoader(dataset=train_dataset,batch_size=batchs,shuffle=True,num_workers=os.cpu_count())\n",
        "\n",
        "  test_loader_simple=DataLoader(dataset=test_dataset,batch_size=batchs,shuffle=False,num_workers=os.cpu_count())\n",
        "\n",
        "  return train_loader_simple,test_loader_simple,len(train_dataset.classes),train_dataset.classes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLyhAwwU6_s",
        "outputId": "01ea19b3-64ed-4135-c7aa-70833527d77f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing out_model_set/data_creator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile out_model_set/torch_customer.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class LossL1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,predict,targets):\n",
        "    return torch.sum(torch.abs(predict - targets))/len(targets)\n",
        "\n",
        "def L1_loss(input, target):\n",
        "    return (target/2).abs().mean()\n",
        "\n",
        "\n",
        "def customer_optimizer(model,learning_rate):\n",
        "    with torch.no_grad():\n",
        "      for p in model.parameters():\n",
        "        p -= p.grad *(learning_rate)\n",
        "      model.zero_grad()\n",
        "\n",
        "\n",
        "class Binary_cross(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "  def forward(self,pred,targets):\n",
        "    #instead: using pred=torch.sigmoid(pred)\n",
        "    pred=(1 + torch.exp(-(pred))).reciprocal()\n",
        "    #build up binary cross-entropy\n",
        "    return torch.mean(-((targets)*torch.log(pred)+(1-targets)*torch.log(1-pred)))\n",
        "\n",
        "class cross_entropy(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "  def one_hot(self,x,y):\n",
        "    return torch.nn.functional.one_hot(x,y)\n",
        "\n",
        "  def softmax_1(self,x):\n",
        "    return (1/torch.sum(torch.exp(x),axis=-1)).unsqueeze(axis=1)*torch.exp(x)\n",
        "   #define the softmax function\n",
        "\n",
        "  def forward(self,pred,targets):\n",
        "    # return the lost function based on the probablity \"pred\" (logits) and y_true targets\n",
        "    targets=self.one_hot(targets.type(torch.long),pred.shape[-1])\n",
        "    pred=self.softmax_1(pred)\n",
        "    sum_one_instance=-1*torch.sum((targets)*torch.log(pred),axis=-1)\n",
        "\n",
        "    return torch.mean(sum_one_instance)\n",
        "\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ReLU1(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.max(torch.tensor([0]).to(device),x)\n",
        "\n",
        "\n",
        "def Sigmoid(x):\n",
        "  #build up the sigmoid function\n",
        "  return (1 + torch.exp(-(x))).reciprocal()\n",
        "\n",
        "class linear1(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "\n",
        "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        torch.nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        return input@self.weight.t()+self.bias\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfIloftYGfkQ",
        "outputId": "7835a106-1f03-4cbd-83c7-7a345a76008e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing out_model_set/torch_customer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3wmMFDgie9Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb45b5af-6a82-4ddf-891c-d9df324405ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/out_model_set/model_creator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/out_model_set/model_creator.py\n",
        "import torch_customer\n",
        "from torch import nn\n",
        "class tinygvv(nn.Module):\n",
        "  def __init__(self,in_fea,out_fea,hidden1,hidden2,hidden3,hidden4,hidden5,hidden6):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer1=nn.Sequential(nn.Conv2d(in_channels=in_fea,out_channels=hidden1,kernel_size=3,stride=1,padding=0),\n",
        "                              torch_customer.ReLU1(),nn.Conv2d(in_channels=hidden1,out_channels=hidden2,kernel_size=3,stride=1,padding=0),torch_customer.ReLU1(),nn.MaxPool2d(2))\n",
        "\n",
        "    self.layer2=nn.Sequential(nn.Conv2d(in_channels=hidden2,out_channels=hidden3,kernel_size=3,stride=1,padding=0),\n",
        "                              torch_customer.ReLU1(),nn.Conv2d(in_channels=hidden3,out_channels=hidden4,kernel_size=3,stride=1,padding=0),torch_customer.ReLU1(),nn.MaxPool2d(2))\n",
        "\n",
        "    self.layer3=nn.Sequential(nn.Conv2d(in_channels=hidden4,out_channels=hidden5,kernel_size=3,stride=1,padding=0),\n",
        "                              torch_customer.ReLU1(),nn.Conv2d(in_channels=hidden5,out_channels=hidden6,kernel_size=3,stride=1,padding=0),torch_customer.ReLU1(),nn.MaxPool2d(2))\n",
        "\n",
        "    self.layer4=nn.Sequential(nn.Flatten(),torch_customer.linear1(5000,out_fea)) #turn into feature vetcors size\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layer4(self.layer3(self.layer2(self.layer1(x))))   #not go back to computation not in one hint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Ur8fnB5Bzao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd0b0c6-c8f8-4c99-c9fb-636457da7c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/out_model_set/train_creator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/out_model_set/train_creator.py\n",
        "#train_loader=DataLoader(dataset=train_dataset,batch_size=32,num_workers=2,shuffle=True)\n",
        "#test_loader=DataLoader(dataset=test_data,batch_size=32,num_workers=2,shuffle=False)\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model,data_loader,lost_fn,optimizer,device=device):\n",
        "  model.train()\n",
        "  loss=0\n",
        "  acc=0\n",
        "  for batch,(X,y) in enumerate(data_loader):\n",
        "    X=X.to(device)\n",
        "    y=y.to(device)\n",
        "    y_logit=model(X)\n",
        "\n",
        "    y_label=torch.softmax(y_logit,-1).argmax(-1)\n",
        "\n",
        "    loss_ff=lost_fn(y_logit,y)\n",
        "\n",
        "    loss=loss+loss_ff.item()\n",
        "\n",
        "    acc=acc+torch.sum(y_label==y).item()/len(y)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_ff.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  loss=loss/len(data_loader)\n",
        "\n",
        "  acc=acc/len(data_loader)\n",
        "\n",
        "  return loss,acc\n",
        "\n",
        "def test_step(model,data_loader,lost_fn,device=device):\n",
        "  model.eval()\n",
        "  loss=0\n",
        "  acc=0\n",
        "  with torch.inference_mode():\n",
        "    for batch,(X,y) in enumerate(data_loader):\n",
        "      X=X.to(device)\n",
        "      y=y.to(device)\n",
        "      y_logit=model(X)\n",
        "\n",
        "      y_label=torch.softmax(y_logit,-1).argmax(-1)\n",
        "\n",
        "      loss_ff=lost_fn(y_logit,y)\n",
        "      loss=loss+loss_ff.item()\n",
        "\n",
        "      acc=acc+torch.sum(y_label==y).item()/len(y)\n",
        "\n",
        "    loss=loss/len(data_loader)\n",
        "\n",
        "    acc=acc/len(data_loader)\n",
        "\n",
        "  return loss,acc\n",
        "\n",
        "\n",
        "\n",
        "def model_fit(model,epochs,train_loaders,test_loaders,loss_fun,optimizer,device=device):\n",
        "\n",
        "  training_loss=[]\n",
        "  training_acc=[]\n",
        "  testing_loss=[]\n",
        "  testing_acc=[]\n",
        "\n",
        "  for i in range(epochs):\n",
        "    a,b=train_step(model,train_loaders,loss_fun,optimizer,device=device)\n",
        "\n",
        "\n",
        "    c,d=test_step(model,test_loaders,loss_fun,device=device)\n",
        "    training_loss.append(a)\n",
        "    training_acc.append(b)\n",
        "    testing_loss.append(c)\n",
        "    testing_acc.append(d)\n",
        "    print(f\"training_loss : {a}, training_acc : {b},testing_loss : {c}, testing_acc : {d}\")\n",
        "\n",
        "  return {\"training_loss\": training_loss,\"training acc\": training_acc,\"testing_loss\": testing_loss,\"testing acc\": testing_acc}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile out_model_set/final_train.py\n",
        "#train_loader=DataLoader(dataset=train_dataset,batch_size=32,num_workers=2,shuffle=True)\n",
        "#test_loader=DataLoader(dataset=test_data,batch_size=32,num_workers=2,shuffle=False)\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import torch_customer\n",
        "import data_creator,model_creator,train_creator\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from torch import nn\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description=\"This is example of argparse!\")\n",
        "parser.add_argument('--BATCH_size',default=32,type=int)\n",
        "parser.add_argument(\"--hidden_unit1\",default=256,type=int)\n",
        "parser.add_argument(\"--hidden_unit2\",default=128,type=int)\n",
        "parser.add_argument(\"--hidden_unit3\",default=64,type=int)\n",
        "parser.add_argument(\"--hidden_unit4\",default=32,type=int)\n",
        "parser.add_argument(\"--hidden_unit5\",default=16,type=int)\n",
        "\n",
        "parser.add_argument('--NUM_EPOCHS',default=5,type=int)\n",
        "parser.add_argument('--Learning_rate',default=0.001,type=float)\n",
        "parser.add_argument(\"--imagepath\",default=\"/content/Brain-Tumor-Classification-DataSet\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "imagepath=args.imagepath\n",
        "hidden_unit1=args.hidden_unit1\n",
        "hidden_unit2=args.hidden_unit2\n",
        "hidden_unit3=args.hidden_unit3\n",
        "hidden_unit4=args.hidden_unit4\n",
        "hidden_unit5=args.hidden_unit5\n",
        "batch=args.BATCH_size\n",
        "NUM_EPOCHS =args.NUM_EPOCHS\n",
        "Learning_rate=args.Learning_rate\n",
        "\n",
        "imagepath=Path(imagepath)\n",
        "\n",
        "train_dir=imagepath/\"Training\"\n",
        "test_dir=imagepath/\"Testing\"\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_loader_simple,test_loader_simple,class_num,class_name=data_creator.customdataset_loader(batch,train_dir,test_dir)\n",
        "\n",
        "loss_fn=torch_customer.cross_entropy()\n",
        "\n",
        "\n",
        "model_0=model_creator.tinygvv(3,class_num,hidden_unit1,hidden_unit2,hidden_unit3,hidden_unit4,hidden_unit5,8).to(device)\n",
        "\n",
        "\n",
        "optimizer_0=torch.optim.Adam(model_0.parameters(), lr=Learning_rate)\n",
        "\n",
        "model_fit_1=train_creator.model_fit(model_0,NUM_EPOCHS,train_loader_simple,test_loader_simple,loss_fn,optimizer_0)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"MODEL_PATH222\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "\n",
        "torch.save(model_0.state_dict(),f=MODEL_SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "8ZiDxExOWO00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8087ae-9bb5-4eb1-ee5f-55146582c1cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing out_model_set/final_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile open_test_2.py\n",
        "\n",
        "import requests\n",
        "request=requests.get('https://cdnintech.com/media/chapter/43882/1512345123/media/image3.png')\n",
        "#https://www.intechopen.com/chapters/43882\n",
        "#request=requests.get('https://cdn.the-scientist.com/assets/articleNo/70327/iImg/47139/glioblastoma-inline-l.webp')\n",
        "with open(\"/content/Brain-Tumor-Classification-DataSet/test_image2\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "#import requests\n",
        "#import Image\n",
        "#Image.open(MODEL_PATH/\"test_image2\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3aJCwIhBAPK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab9e168-97e3-40e9-ab8f-86418365ad3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing open_test_2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile out_model_set/final_pred.py\n",
        "\n",
        "\n",
        "from torchvision import  transforms\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import model_creator,data_creator\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description=\"This is example of argparse!\")\n",
        "parser.add_argument('--imagepath',default=\"test_image2\")\n",
        "parser.add_argument(\"--hidden_unit1\",default=256,type=int)\n",
        "parser.add_argument(\"--hidden_unit2\",default=128,type=int)\n",
        "parser.add_argument(\"--hidden_unit3\",default=64,type=int)\n",
        "parser.add_argument(\"--hidden_unit4\",default=32,type=int)\n",
        "parser.add_argument(\"--hidden_unit5\",default=16,type=int)\n",
        "\n",
        "parser.add_argument('--NUM_EPOCHS',default=5,type=int)\n",
        "parser.add_argument('--Learning_rate',default=0.001,type=float)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "imagepath=args.imagepath\n",
        "hidden_unit1=args.hidden_unit1\n",
        "hidden_unit2=args.hidden_unit2\n",
        "hidden_unit3=args.hidden_unit3\n",
        "hidden_unit4=args.hidden_unit4\n",
        "hidden_unit5=args.hidden_unit5\n",
        "\n",
        "def convertion_function(path0):\n",
        "\n",
        "  device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  newImagePath=Path(\"/content/Brain-Tumor-Classification-DataSet\")\n",
        "\n",
        "  custom_uni8=torchvision.io.read_image(newImagePath/path0)\n",
        "  custom_uni8=custom_uni8.type(torch.float).to(device)/255\n",
        "\n",
        "  return custom_uni8\n",
        "\n",
        "def custom_loader(path0=imagepath):\n",
        "\n",
        "  data_transform_simple=transforms.Compose(\n",
        "    [transforms.Resize((228,228))])\n",
        "\n",
        "  image=data_transform_simple(convertion_function(path0))\n",
        "  image=image.unsqueeze(0)\n",
        "\n",
        "\n",
        "  device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  load_path=Path(\"/content/models/MODEL_PATH222\")\n",
        "\n",
        "\n",
        "  load_model=model_creator.tinygvv(3,4,hidden_unit1,hidden_unit2,hidden_unit3,hidden_unit4,hidden_unit5,8).to(device)\n",
        "\n",
        "\n",
        "  load_model.load_state_dict(torch.load(f=load_path,map_location=torch.device('cpu')))\n",
        "\n",
        "  load_model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    y_logit=load_model(image)\n",
        "\n",
        "    y_label=torch.softmax(y_logit,-1).argmax(-1)\n",
        "\n",
        "  return y_label.item()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print (custom_loader())\n",
        "\n"
      ],
      "metadata": {
        "id": "YwfMy7GVBKwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2061f53-c242-4dab-f23a-f3ada3c74a7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing out_model_set/final_pred.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/SartajBhuvaji/Brain-Tumor-Classification-DataSet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bgHGyOcqmpz",
        "outputId": "973a52d3-2d7f-4d5d-d0b5-cb6cdca7d663"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Brain-Tumor-Classification-DataSet'...\n",
            "remote: Enumerating objects: 3039, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 3039 (delta 0), reused 0 (delta 0), pack-reused 3035\u001b[K\n",
            "Receiving objects: 100% (3039/3039), 79.25 MiB | 13.56 MiB/s, done.\n",
            "Updating files: 100% (3264/3264), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python open_test_2.py"
      ],
      "metadata": {
        "id": "8hAjcrXFqnTr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm open_test_2.py"
      ],
      "metadata": {
        "id": "P1aswrQvCvIW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm path_loader.py"
      ],
      "metadata": {
        "id": "0HNlmB1bC2u1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python out_model_set/final_train.py --NUM_EPOCHS 10 --hidden_unit1 256 --hidden_unit2  256 --hidden_unit3 128 --hidden_unit4 128  --hidden_unit5 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxctAqBppZeN",
        "outputId": "58ff2657-33a2-42fa-bd58-933aa70a23cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 7.15 GiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "training_loss : 1.2565650039248997, training_acc : 0.41622474747474747,testing_loss : 2.051750829586616, testing_acc : 0.26778846153846153\n",
            "training_loss : 1.0105082445674471, training_acc : 0.561584595959596,testing_loss : 1.747493354173807, testing_acc : 0.3591346153846154\n",
            "training_loss : 0.9027382029427422, training_acc : 0.6277146464646465,testing_loss : 1.5375316716157472, testing_acc : 0.4225961538461539\n",
            "training_loss : 0.8801874028311836, training_acc : 0.6402146464646464,testing_loss : 1.3655330194876745, testing_acc : 0.5139423076923078\n",
            "training_loss : 0.8088463313049741, training_acc : 0.6706123737373737,testing_loss : 1.503236974661167, testing_acc : 0.5644230769230769\n",
            "training_loss : 0.7816473874780867, training_acc : 0.6882891414141414,testing_loss : 1.59035385457369, testing_acc : 0.6081730769230769\n",
            "training_loss : 0.7617087850968043, training_acc : 0.7070391414141414,testing_loss : 1.4992044178339152, testing_acc : 0.5764423076923078\n",
            "training_loss : 0.7097076853116353, training_acc : 0.7119002525252526,testing_loss : 1.361608007779488, testing_acc : 0.5740384615384616\n",
            "training_loss : 0.6910121106439167, training_acc : 0.720391414141414,testing_loss : 1.7609556638277495, testing_acc : 0.6509615384615385\n",
            "training_loss : 0.6774514300955666, training_acc : 0.734280303030303,testing_loss : 1.5163075098624597, testing_acc : 0.6533653846153846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python out_model_set/final_pred.py  --hidden_unit1 128 --hidden_unit2  64 --hidden_unit3 32 --hidden_unit4 32  --hidden_unit5 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfL_FqHQame_",
        "outputId": "5bab168e-8562-4577-f3e9-bd5e1fbfd071"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python out_model_set/final_pred.py  --imagepath \"Testing/pituitary_tumor/image(54).jpg\" --hidden_unit1 128 --hidden_unit2  64 --hidden_unit3 32 --hidden_unit4 32  --hidden_unit5 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsRdvAfnEkqe",
        "outputId": "23bd4f54-1d6f-42ce-ea12-64f71b647e2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torchvision\n",
        "\n",
        "print (torchvision.__version__,torch.__version__)"
      ],
      "metadata": {
        "id": "ACLYcEmdUewT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd56a1b-2fdc-4301-9e43-a1889a521051"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.18.0+cu121 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from out_model_set import data_creator,model_creator,train_creator\n",
        "\n"
      ],
      "metadata": {
        "id": "i5zNIR_f-g51"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_creator.customdataset_loader()"
      ],
      "metadata": {
        "id": "EMwI0qq1Chtk"
      },
      "execution_count": 19,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}