{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garylau1/vertex-minors/blob/master/Complete_script_customerData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b864oO6hpTqX"
      },
      "source": [
        "\n",
        "## In this task we would do a vision classification task in Brain tumor from scatch.\n",
        "\n",
        "### Our result:\n",
        "The accuracy is around 75 in testing set after trying different layers.\n",
        "This model is very well in detecting class 1,2 and 3 (meningioma_tumor,no_tumor nd pituitary_tumour) but fail to detect class 0 (glimoa_tumor).\n",
        "\n",
        "### The most significance is:\n",
        "\n",
        "- We build up our linear layers ,relu layers and loss functions from scatch from basic class objects(e.g writing the whole formula inside the softmax function and cross-entropy loss)\n",
        "\n",
        "\n",
        "- We build up out custom dataset from scatch as well.\n",
        "\n",
        "\n",
        "- We write into python scripts and run it directly using command line in colab.\n",
        "\n",
        "### Future work\n",
        "\n",
        "- We might try transfer learning\n",
        "- We can try different paraemeters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrR8mBbuLLMV",
        "outputId": "fec7d2ab-7f95-4362-9792-7a748d9daa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing path_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile path_loader.py\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "out_path=Path(\"out_model_set\")\n",
        "os.makedirs(out_path,exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "Imagepath=(\"/content/Brain-Tumor-Classification-DataSet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvEXqUnRr9UM"
      },
      "outputs": [],
      "source": [
        "!python path_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLyhAwwU6_s",
        "outputId": "b3fb77dc-c6f2-4856-a05c-60c05bef983d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing out_model_set/data_creator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile out_model_set/data_creator.py\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_classes(dict1):\n",
        "  \"\"\"find the class fikder bane\"\"\"\n",
        "\n",
        "  found=sorted([entry1.name for entry1 in list(os.scandir(dict1))if entry1.is_dir()])\n",
        "  if not found:\n",
        "      raise  FileNotFoundError(f\"no such class1111 in {dict1}\")\n",
        "\n",
        "  class_index={class_name: i for i,class_name in enumerate(found)}\n",
        "  return found,class_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class customdataset(Dataset):\n",
        "\n",
        "  def find_classes(self,dict1):\n",
        "\n",
        "    found=sorted([entry1.name for entry1 in list(os.scandir(dict1))if entry1.is_dir()])\n",
        "    #found=[entry1.name for entry1 in list(os.scandir(dict1))if entry1.is_dir()]\n",
        "    if not found:\n",
        "      raise  FileNotFoundError(f\"no such class1111 in {dict1}\")\n",
        "\n",
        "    class_index={class_name: i for i,class_name in enumerate(found)}\n",
        "    return found,class_index\n",
        "\n",
        "\n",
        "  def __init__(self,target_folder,transform=None):\n",
        "    #self.targetpath=list(target_folder.glob(\"*/*.jpg\"))\n",
        "    self.targetpath=sorted(list(pathlib.Path(target_folder).glob(\"*/*.jpg\")))\n",
        "    self.transform=transform\n",
        "    self.classes,self.class_to_idx=self.find_classes(target_folder)\n",
        "\n",
        "    self.transform_2=transforms.Compose(\n",
        "    [transforms.Resize((224,224)),transforms.RandomHorizontalFlip(p=0.5),transforms.ToTensor()])\n",
        "\n",
        "\n",
        "  def load_image(self,index):\n",
        "      return (Image.open(self.targetpath[index]))\n",
        "  def __len__(self):\n",
        "      a=self.targetpath\n",
        "      return len(a)\n",
        "\n",
        "  def __getitem__(self,inx):\n",
        "\n",
        "    load=(self.load_image(inx))\n",
        "    name=self.targetpath[inx].parent.name\n",
        "    class_index=self.class_to_idx[name]\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(load),class_index\n",
        "    else:\n",
        "      return self.transform_2(load),class_index\n",
        "\n",
        "\n",
        "data_transform_aug=transforms.Compose([transforms.Resize((228,228)),transforms.TrivialAugmentWide(num_magnitude_bins=31),transforms.ToTensor()])\n",
        "\n",
        "data_transform_simple=transforms.Compose(\n",
        "    [transforms.Resize((228,228)),transforms.ToTensor()])\n",
        "\n",
        "def customdataset_loader(batchs,train_dir,test_dir,transform=data_transform_aug,transform2=data_transform_simple):\n",
        "  train_dataset=customdataset(train_dir,data_transform_aug)\n",
        "  test_dataset=customdataset(test_dir,data_transform_simple)\n",
        "\n",
        "  class_num=len(train_dataset.classes)\n",
        "\n",
        "  train_loader_simple=DataLoader(dataset=train_dataset,batch_size=batchs,shuffle=True,num_workers=os.cpu_count())\n",
        "\n",
        "  test_loader_simple=DataLoader(dataset=test_dataset,batch_size=batchs,shuffle=False,num_workers=os.cpu_count())\n",
        "\n",
        "  return train_loader_simple,test_loader_simple,len(train_dataset.classes),train_dataset.classes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfIloftYGfkQ",
        "outputId": "90180830-9856-4b51-cd21-acd37d2602eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing out_model_set/torch_customer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile out_model_set/torch_customer.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class LossL1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,predict,targets):\n",
        "    return torch.sum(torch.abs(predict - targets))/len(targets)\n",
        "\n",
        "def L1_loss(input, target):\n",
        "    return (target/2).abs().mean()\n",
        "\n",
        "\n",
        "def customer_optimizer(model,learning_rate):\n",
        "    with torch.no_grad():\n",
        "      for p in model.parameters():\n",
        "        p -= p.grad *(learning_rate)\n",
        "      model.zero_grad()\n",
        "\n",
        "\n",
        "class Binary_cross(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "  def forward(self,pred,targets):\n",
        "    #instead: using pred=torch.sigmoid(pred)\n",
        "    pred=(1 + torch.exp(-(pred))).reciprocal()\n",
        "    #build up binary cross-entropy\n",
        "    return torch.mean(-((targets)*torch.log(pred)+(1-targets)*torch.log(1-pred)))\n",
        "\n",
        "class cross_entropy(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "  def one_hot(self,x,y):\n",
        "    return torch.nn.functional.one_hot(x,y)\n",
        "\n",
        "  def softmax_1(self,x):\n",
        "    return (1/torch.sum(torch.exp(x),axis=-1)).unsqueeze(axis=1)*torch.exp(x)\n",
        "   #define the softmax function\n",
        "\n",
        "  def forward(self,pred,targets):\n",
        "    # return the lost function based on the probablity \"pred\" (logits) and y_true targets\n",
        "    targets=self.one_hot(targets.type(torch.long),pred.shape[-1])\n",
        "    pred=self.softmax_1(pred)\n",
        "    sum_one_instance=-1*torch.sum((targets)*torch.log(pred),axis=-1)\n",
        "\n",
        "    return torch.mean(sum_one_instance)\n",
        "\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ReLU1(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.max(torch.tensor([0]).to(device),x)\n",
        "\n",
        "\n",
        "def Sigmoid(x):\n",
        "  #build up the sigmoid function\n",
        "  return (1 + torch.exp(-(x))).reciprocal()\n",
        "\n",
        "class linear1(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "\n",
        "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        torch.nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        return input@self.weight.t()+self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wmMFDgie9Pb",
        "outputId": "74c0aa62-0c23-4c1a-80c4-cea34be3799b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/out_model_set/model_creator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/out_model_set/model_creator.py\n",
        "import torch_customer\n",
        "from torch import nn\n",
        "class tinygvv(nn.Module):\n",
        "  def __init__(self,in_fea,out_fea,hidden1,hidden2,hidden3,hidden4,hidden5,hidden6):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer1=nn.Sequential(nn.Conv2d(in_channels=in_fea,out_channels=hidden1,kernel_size=3,stride=1,padding=0),\n",
        "                              torch_customer.ReLU1(),nn.Conv2d(in_channels=hidden1,out_channels=hidden2,kernel_size=3,stride=1,padding=0),torch_customer.ReLU1(),nn.MaxPool2d(2))\n",
        "\n",
        "    self.layer2=nn.Sequential(nn.Conv2d(in_channels=hidden2,out_channels=hidden3,kernel_size=3,stride=1,padding=0),\n",
        "                              torch_customer.ReLU1(),nn.Conv2d(in_channels=hidden3,out_channels=hidden4,kernel_size=3,stride=1,padding=0),torch_customer.ReLU1(),nn.MaxPool2d(2))\n",
        "\n",
        "    self.layer3=nn.Sequential(nn.Conv2d(in_channels=hidden4,out_channels=hidden5,kernel_size=3,stride=1,padding=0),\n",
        "                              torch_customer.ReLU1(),nn.Conv2d(in_channels=hidden5,out_channels=hidden6,kernel_size=3,stride=1,padding=0),torch_customer.ReLU1(),nn.MaxPool2d(2))\n",
        "\n",
        "    self.layer4=nn.Sequential(nn.Flatten(),torch_customer.linear1(5000,out_fea)) #turn into feature vetcors size\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layer4(self.layer3(self.layer2(self.layer1(x))))   #not go back to computation not in one hint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ur8fnB5Bzao",
        "outputId": "b7dc3cc0-9a6b-4934-b79e-7a61e2f3c860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/out_model_set/train_creator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/out_model_set/train_creator.py\n",
        "#train_loader=DataLoader(dataset=train_dataset,batch_size=32,num_workers=2,shuffle=True)\n",
        "#test_loader=DataLoader(dataset=test_data,batch_size=32,num_workers=2,shuffle=False)\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model,data_loader,lost_fn,optimizer,device=device):\n",
        "  model.train()\n",
        "  loss=0\n",
        "  acc=0\n",
        "  for batch,(X,y) in enumerate(data_loader):\n",
        "    X=X.to(device)\n",
        "    y=y.to(device)\n",
        "    y_logit=model(X)\n",
        "\n",
        "    y_label=torch.softmax(y_logit,-1).argmax(-1)\n",
        "\n",
        "    loss_ff=lost_fn(y_logit,y)\n",
        "\n",
        "    loss=loss+loss_ff.item()\n",
        "\n",
        "    acc=acc+torch.sum(y_label==y).item()/len(y)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_ff.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  loss=loss/len(data_loader)\n",
        "\n",
        "  acc=acc/len(data_loader)\n",
        "\n",
        "  return loss,acc\n",
        "\n",
        "def test_step(model,data_loader,lost_fn,device=device):\n",
        "  model.eval()\n",
        "  loss=0\n",
        "  acc=0\n",
        "  with torch.inference_mode():\n",
        "    for batch,(X,y) in enumerate(data_loader):\n",
        "      X=X.to(device)\n",
        "      y=y.to(device)\n",
        "      y_logit=model(X)\n",
        "\n",
        "      y_label=torch.softmax(y_logit,-1).argmax(-1)\n",
        "\n",
        "      loss_ff=lost_fn(y_logit,y)\n",
        "      loss=loss+loss_ff.item()\n",
        "\n",
        "      acc=acc+torch.sum(y_label==y).item()/len(y)\n",
        "\n",
        "    loss=loss/len(data_loader)\n",
        "\n",
        "    acc=acc/len(data_loader)\n",
        "\n",
        "  return loss,acc\n",
        "\n",
        "\n",
        "\n",
        "def model_fit(model,epochs,train_loaders,test_loaders,loss_fun,optimizer,device=device):\n",
        "\n",
        "  training_loss=[]\n",
        "  training_acc=[]\n",
        "  testing_loss=[]\n",
        "  testing_acc=[]\n",
        "\n",
        "  for i in range(epochs):\n",
        "    a,b=train_step(model,train_loaders,loss_fun,optimizer,device=device)\n",
        "\n",
        "\n",
        "    c,d=test_step(model,test_loaders,loss_fun,device=device)\n",
        "    training_loss.append(a)\n",
        "    training_acc.append(b)\n",
        "    testing_loss.append(c)\n",
        "    testing_acc.append(d)\n",
        "    print(f\"training_loss : {a}, training_acc : {b},testing_loss : {c}, testing_acc : {d}\")\n",
        "\n",
        "  return {\"training_loss\": training_loss,\"training acc\": training_acc,\"testing_loss\": testing_loss,\"testing acc\": testing_acc}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZiDxExOWO00",
        "outputId": "e84caca6-b540-458b-c024-fc433ae04232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing out_model_set/final_train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile out_model_set/final_train.py\n",
        "#train_loader=DataLoader(dataset=train_dataset,batch_size=32,num_workers=2,shuffle=True)\n",
        "#test_loader=DataLoader(dataset=test_data,batch_size=32,num_workers=2,shuffle=False)\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import torch_customer\n",
        "import data_creator,model_creator,train_creator\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from torch import nn\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description=\"This is example of argparse!\")\n",
        "parser.add_argument('--BATCH_size',default=32,type=int)\n",
        "parser.add_argument(\"--hidden_unit1\",default=256,type=int)\n",
        "parser.add_argument(\"--hidden_unit2\",default=128,type=int)\n",
        "parser.add_argument(\"--hidden_unit3\",default=64,type=int)\n",
        "parser.add_argument(\"--hidden_unit4\",default=32,type=int)\n",
        "parser.add_argument(\"--hidden_unit5\",default=16,type=int)\n",
        "\n",
        "parser.add_argument('--NUM_EPOCHS',default=5,type=int)\n",
        "parser.add_argument('--Learning_rate',default=0.001,type=float)\n",
        "parser.add_argument(\"--imagepath\",default=\"/content/Brain-Tumor-Classification-DataSet\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "imagepath=args.imagepath\n",
        "hidden_unit1=args.hidden_unit1\n",
        "hidden_unit2=args.hidden_unit2\n",
        "hidden_unit3=args.hidden_unit3\n",
        "hidden_unit4=args.hidden_unit4\n",
        "hidden_unit5=args.hidden_unit5\n",
        "batch=args.BATCH_size\n",
        "NUM_EPOCHS =args.NUM_EPOCHS\n",
        "Learning_rate=args.Learning_rate\n",
        "\n",
        "imagepath=Path(imagepath)\n",
        "\n",
        "train_dir=imagepath/\"Training\"\n",
        "test_dir=imagepath/\"Testing\"\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_loader_simple,test_loader_simple,class_num,class_name=data_creator.customdataset_loader(batch,train_dir,test_dir)\n",
        "\n",
        "loss_fn=torch_customer.cross_entropy()\n",
        "\n",
        "\n",
        "model_0=model_creator.tinygvv(3,class_num,hidden_unit1,hidden_unit2,hidden_unit3,hidden_unit4,hidden_unit5,8).to(device)\n",
        "\n",
        "\n",
        "optimizer_0=torch.optim.Adam(model_0.parameters(), lr=Learning_rate)\n",
        "\n",
        "model_fit_1=train_creator.model_fit(model_0,NUM_EPOCHS,train_loader_simple,test_loader_simple,loss_fn,optimizer_0)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"MODEL_PATH222\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "\n",
        "torch.save(model_0.state_dict(),f=MODEL_SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aJCwIhBAPK3",
        "outputId": "43d88feb-3fca-4693-ee08-4743328af618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing open_test_2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile open_test_2.py\n",
        "\n",
        "import requests\n",
        "request=requests.get('https://cdnintech.com/media/chapter/43882/1512345123/media/image3.png')\n",
        "#https://www.intechopen.com/chapters/43882\n",
        "#request=requests.get('https://cdn.the-scientist.com/assets/articleNo/70327/iImg/47139/glioblastoma-inline-l.webp')\n",
        "with open(\"/content/Brain-Tumor-Classification-DataSet/test_image2\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "#import requests\n",
        "#import Image\n",
        "#Image.open(MODEL_PATH/\"test_image2\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwfMy7GVBKwl",
        "outputId": "7e0da3cf-02e5-480d-a567-f06f350c2f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing out_model_set/final_pred.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile out_model_set/final_pred.py\n",
        "\n",
        "\n",
        "from torchvision import  transforms\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import model_creator,data_creator\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description=\"This is example of argparse!\")\n",
        "parser.add_argument('--imagepath',default=\"test_image2\")\n",
        "parser.add_argument(\"--hidden_unit1\",default=256,type=int)\n",
        "parser.add_argument(\"--hidden_unit2\",default=128,type=int)\n",
        "parser.add_argument(\"--hidden_unit3\",default=64,type=int)\n",
        "parser.add_argument(\"--hidden_unit4\",default=32,type=int)\n",
        "parser.add_argument(\"--hidden_unit5\",default=16,type=int)\n",
        "\n",
        "parser.add_argument('--NUM_EPOCHS',default=5,type=int)\n",
        "parser.add_argument('--Learning_rate',default=0.001,type=float)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "imagepath=args.imagepath\n",
        "hidden_unit1=args.hidden_unit1\n",
        "hidden_unit2=args.hidden_unit2\n",
        "hidden_unit3=args.hidden_unit3\n",
        "hidden_unit4=args.hidden_unit4\n",
        "hidden_unit5=args.hidden_unit5\n",
        "\n",
        "def convertion_function(path0):\n",
        "\n",
        "  device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  newImagePath=Path(\"/content/Brain-Tumor-Classification-DataSet\")\n",
        "\n",
        "  custom_uni8=torchvision.io.read_image(newImagePath/path0)\n",
        "  custom_uni8=custom_uni8.type(torch.float).to(device)/255\n",
        "\n",
        "  return custom_uni8\n",
        "\n",
        "def custom_loader(path0=imagepath):\n",
        "\n",
        "  data_transform_simple=transforms.Compose(\n",
        "    [transforms.Resize((228,228))])\n",
        "\n",
        "  image=data_transform_simple(convertion_function(path0))\n",
        "  image=image.unsqueeze(0)\n",
        "\n",
        "\n",
        "  device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  load_path=Path(\"/content/models/MODEL_PATH222\")\n",
        "\n",
        "\n",
        "  load_model=model_creator.tinygvv(3,4,hidden_unit1,hidden_unit2,hidden_unit3,hidden_unit4,hidden_unit5,8).to(device)\n",
        "\n",
        "\n",
        "  load_model.load_state_dict(torch.load(f=load_path,map_location=torch.device('cpu')))\n",
        "\n",
        "  load_model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    y_logit=load_model(image)\n",
        "\n",
        "    y_label=torch.softmax(y_logit,-1).argmax(-1)\n",
        "\n",
        "  return y_label.item()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print (custom_loader())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bgHGyOcqmpz",
        "outputId": "94e41c41-54e3-42a5-dffc-f20bd803b72e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Brain-Tumor-Classification-DataSet'...\n",
            "remote: Enumerating objects: 3039, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 3039 (delta 0), reused 0 (delta 0), pack-reused 3035\u001b[K\n",
            "Receiving objects: 100% (3039/3039), 79.25 MiB | 15.28 MiB/s, done.\n",
            "Updating files: 100% (3264/3264), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/SartajBhuvaji/Brain-Tumor-Classification-DataSet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hAjcrXFqnTr"
      },
      "outputs": [],
      "source": [
        "!python open_test_2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1aswrQvCvIW"
      },
      "outputs": [],
      "source": [
        "!rm open_test_2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HNlmB1bC2u1"
      },
      "outputs": [],
      "source": [
        "!rm path_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxctAqBppZeN",
        "outputId": "2ff558db-4dd3-49eb-b592-d57e78c9ddd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training_loss : 1.3004170603222318, training_acc : 0.3658459595959596,testing_loss : 1.4495824575424194, testing_acc : 0.3067307692307692\n",
            "training_loss : 1.1258316814899445, training_acc : 0.49712752525252524,testing_loss : 1.7830489254914796, testing_acc : 0.4009615384615385\n",
            "training_loss : 0.9551391522089641, training_acc : 0.5879419191919192,testing_loss : 1.8710358486725733, testing_acc : 0.4153846153846154\n",
            "training_loss : 0.8546420249674055, training_acc : 0.6400252525252526,testing_loss : 2.0982380417677073, testing_acc : 0.4480769230769231\n",
            "training_loss : 0.8050029483106401, training_acc : 0.6732007575757576,testing_loss : 1.966814609674307, testing_acc : 0.49230769230769234\n",
            "training_loss : 0.7703803658485413, training_acc : 0.6843434343434344,testing_loss : 1.734482785830131, testing_acc : 0.4947115384615385\n",
            "training_loss : 0.7486185007625156, training_acc : 0.7024936868686869,testing_loss : 1.7446269255418043, testing_acc : 0.47692307692307695\n",
            "training_loss : 0.7379559871223238, training_acc : 0.7038825757575757,testing_loss : 1.695035138955483, testing_acc : 0.5475961538461539\n",
            "training_loss : 0.6945359352562163, training_acc : 0.7223484848484849,testing_loss : 1.8939127853283515, testing_acc : 0.5899038461538462\n",
            "training_loss : 0.6330147640572654, training_acc : 0.749715909090909,testing_loss : 1.4852754588310535, testing_acc : 0.6081730769230769\n",
            "training_loss : 0.6087922101219495, training_acc : 0.7606376262626262,testing_loss : 1.9910347398657064, testing_acc : 0.6360576923076924\n",
            "training_loss : 0.5848734392060174, training_acc : 0.7762626262626262,testing_loss : 1.7965850898852715, testing_acc : 0.6091346153846154\n",
            "training_loss : 0.5699272841215134, training_acc : 0.7733901515151516,testing_loss : 1.4531146998588855, testing_acc : 0.6033653846153846\n",
            "training_loss : 0.57946873638365, training_acc : 0.781344696969697,testing_loss : 1.7062072157859802, testing_acc : 0.6504807692307693\n",
            "training_loss : 0.5407999376455943, training_acc : 0.7915719696969696,testing_loss : 1.7240688766424472, testing_acc : 0.6418269230769231\n",
            "training_loss : 0.5473639594184028, training_acc : 0.7908775252525252,testing_loss : 1.5531961344755614, testing_acc : 0.6499999999999999\n",
            "training_loss : 0.5291621241304609, training_acc : 0.8064709595959596,testing_loss : 1.5322231031381166, testing_acc : 0.6442307692307693\n",
            "training_loss : 0.5163643361793624, training_acc : 0.8035669191919192,testing_loss : 1.4602530953975825, testing_acc : 0.6913461538461539\n",
            "training_loss : 0.48746487663851845, training_acc : 0.8111742424242424,testing_loss : 1.8588092143719013, testing_acc : 0.6889423076923078\n",
            "training_loss : 0.5130939195553462, training_acc : 0.8068181818181818,testing_loss : 1.5296044876942267, testing_acc : 0.6716346153846153\n",
            "training_loss : 0.45904623832967545, training_acc : 0.8196969696969696,testing_loss : 1.6185080523674304, testing_acc : 0.6692307692307692\n",
            "training_loss : 0.48866321759091486, training_acc : 0.8105113636363637,testing_loss : 1.4804365904285357, testing_acc : 0.6783653846153845\n",
            "training_loss : 0.45810117903682923, training_acc : 0.825915404040404,testing_loss : 1.4428042872593954, testing_acc : 0.7096153846153845\n",
            "training_loss : 0.4429558830128776, training_acc : 0.8351010101010101,testing_loss : 1.631271522062329, testing_acc : 0.6817307692307693\n",
            "training_loss : 0.4875485089090135, training_acc : 0.8093434343434344,testing_loss : 1.8263159910073647, testing_acc : 0.7153846153846154\n",
            "training_loss : 0.4438342284825113, training_acc : 0.8384469696969696,testing_loss : 1.9210542698319142, testing_acc : 0.7153846153846154\n",
            "training_loss : 0.4760262414813042, training_acc : 0.8183396464646465,testing_loss : 1.5212919906927989, testing_acc : 0.7216346153846154\n",
            "training_loss : 0.44347413761748206, training_acc : 0.830334595959596,testing_loss : 1.5782797835194147, testing_acc : 0.7298076923076924\n",
            "training_loss : 0.4246327352192667, training_acc : 0.8434343434343434,testing_loss : 1.5597171554198632, testing_acc : 0.7451923076923077\n",
            "training_loss : 0.4328915705283483, training_acc : 0.836300505050505,testing_loss : 1.4660474509000778, testing_acc : 0.7336538461538461\n",
            "training_loss : 0.41959814214044144, training_acc : 0.8384469696969696,testing_loss : 1.5167028577281878, testing_acc : 0.7298076923076924\n",
            "training_loss : 0.38391607834233177, training_acc : 0.8560921717171717,testing_loss : 1.5948898270726204, testing_acc : 0.7216346153846154\n",
            "training_loss : 0.39209566795163686, training_acc : 0.8559659090909091,testing_loss : 1.7596574511665564, testing_acc : 0.7120192307692308\n",
            "training_loss : 0.42332839998934, training_acc : 0.838415404040404,testing_loss : 1.5430245691767106, testing_acc : 0.7336538461538461\n",
            "training_loss : 0.3813894908461306, training_acc : 0.8584280303030304,testing_loss : 1.529738336801529, testing_acc : 0.73125\n",
            "training_loss : 0.42069628321462205, training_acc : 0.8460858585858585,testing_loss : 1.5059451300364275, testing_acc : 0.7288461538461538\n",
            "training_loss : 0.4050513663225704, training_acc : 0.850915404040404,testing_loss : 1.7341109262062953, testing_acc : 0.7490384615384615\n",
            "training_loss : 0.3817420375016001, training_acc : 0.8599747474747474,testing_loss : 1.4527655008893747, testing_acc : 0.7514423076923078\n",
            "training_loss : 0.3693651866581705, training_acc : 0.8602588383838383,testing_loss : 1.8886751629985297, testing_acc : 0.739423076923077\n",
            "training_loss : 0.38508164890938335, training_acc : 0.8573232323232323,testing_loss : 1.5834632831124158, testing_acc : 0.7538461538461539\n",
            "training_loss : 0.37316535355316266, training_acc : 0.8568181818181818,testing_loss : 1.6840926431692564, testing_acc : 0.7514423076923078\n",
            "training_loss : 0.36428620111611154, training_acc : 0.860290404040404,testing_loss : 1.5830933679468358, testing_acc : 0.7346153846153847\n",
            "training_loss : 0.35441779866814616, training_acc : 0.8703598484848485,testing_loss : 1.8206075834683502, testing_acc : 0.7370192307692308\n",
            "training_loss : 0.38771939244535236, training_acc : 0.8547348484848485,testing_loss : 1.6019563806744723, testing_acc : 0.7562500000000001\n",
            "training_loss : 0.3465132856534587, training_acc : 0.8722537878787878,testing_loss : 1.6362434313274348, testing_acc : 0.7538461538461539\n",
            "training_loss : 0.385948518746429, training_acc : 0.8547348484848485,testing_loss : 1.576571306357017, testing_acc : 0.7514423076923078\n",
            "training_loss : 0.34327624183562067, training_acc : 0.879040404040404,testing_loss : 1.5526102357185805, testing_acc : 0.7658653846153847\n",
            "training_loss : 0.3358517939845721, training_acc : 0.8722537878787878,testing_loss : 1.7180502509268432, testing_acc : 0.7658653846153847\n",
            "training_loss : 0.3584685754444864, training_acc : 0.8634469696969697,testing_loss : 1.89060485878816, testing_acc : 0.7514423076923078\n",
            "training_loss : 0.3492531654735406, training_acc : 0.8720959595959595,testing_loss : 1.6437305945616503, testing_acc : 0.7610576923076924\n",
            "training_loss : 0.34001944818430474, training_acc : 0.879040404040404,testing_loss : 1.7940483199289212, testing_acc : 0.7346153846153847\n",
            "training_loss : 0.3676108787457148, training_acc : 0.8674242424242423,testing_loss : 1.3821487495532403, testing_acc : 0.7716346153846154\n",
            "training_loss : 0.33357972105344136, training_acc : 0.872790404040404,testing_loss : 1.6869833543896675, testing_acc : 0.7538461538461539\n",
            "training_loss : 0.33172246772381997, training_acc : 0.8840593434343434,testing_loss : 1.5132090747356415, testing_acc : 0.7586538461538462\n",
            "training_loss : 0.36803640491432615, training_acc : 0.8628787878787878,testing_loss : 1.4786095819794214, testing_acc : 0.7514423076923078\n",
            "training_loss : 0.34572938001818126, training_acc : 0.8760732323232323,testing_loss : 1.643195622242414, testing_acc : 0.7740384615384616\n",
            "training_loss : 0.3436925359898143, training_acc : 0.8750315656565656,testing_loss : 1.6059540868378603, testing_acc : 0.7682692307692308\n",
            "training_loss : 0.3350290221472581, training_acc : 0.8734532828282828,testing_loss : 1.6260976997705607, testing_acc : 0.7634615384615385\n",
            "training_loss : 0.31656748238537047, training_acc : 0.8864898989898989,testing_loss : 1.7611360922455788, testing_acc : 0.7538461538461539\n",
            "training_loss : 0.3427691988646984, training_acc : 0.8748737373737373,testing_loss : 1.5802674749149725, testing_acc : 0.7586538461538462\n",
            "training_loss : 0.32779620165626205, training_acc : 0.8781881313131312,testing_loss : 1.7013260809561381, testing_acc : 0.7538461538461539\n",
            "training_loss : 0.32544442754652764, training_acc : 0.8783143939393939,testing_loss : 1.6937245917864716, testing_acc : 0.7562500000000001\n",
            "training_loss : 0.30577756977743575, training_acc : 0.8856376262626262,testing_loss : 1.7263525369075627, testing_acc : 0.7413461538461538\n",
            "training_loss : 0.2840783365898662, training_acc : 0.8972222222222223,testing_loss : 1.7519484525546432, testing_acc : 0.78125\n",
            "training_loss : 0.30607248660590913, training_acc : 0.8877525252525251,testing_loss : 1.629737821335976, testing_acc : 0.7908653846153846\n",
            "training_loss : 0.3083502643638187, training_acc : 0.8864898989898989,testing_loss : 1.5579562078301723, testing_acc : 0.7370192307692308\n",
            "training_loss : 0.32802060138848094, training_acc : 0.8797032828282828,testing_loss : 1.5716935092440019, testing_acc : 0.7514423076923078\n",
            "training_loss : 0.2840525342358483, training_acc : 0.8953598484848484,testing_loss : 1.770009120209859, testing_acc : 0.7370192307692308\n",
            "training_loss : 0.3434006856547462, training_acc : 0.8778093434343434,testing_loss : 1.6175053626872027, testing_acc : 0.7730769230769231\n",
            "training_loss : 0.3034180015739467, training_acc : 0.8816287878787878,testing_loss : 1.5830481313169003, testing_acc : 0.7562500000000001\n",
            "training_loss : 0.28287568464875223, training_acc : 0.8969381313131313,testing_loss : 1.750734224342383, testing_acc : 0.7692307692307693\n",
            "training_loss : 0.3157564404937956, training_acc : 0.8828598484848484,testing_loss : 1.6489744298160076, testing_acc : 0.7788461538461539\n",
            "training_loss : 0.2855640453182989, training_acc : 0.8951388888888889,testing_loss : 1.953199046305739, testing_acc : 0.7692307692307693\n",
            "training_loss : 0.3159553435113695, training_acc : 0.8840593434343434,testing_loss : 1.63767075080138, testing_acc : 0.7591346153846155\n",
            "training_loss : 0.290937234626876, training_acc : 0.8941919191919193,testing_loss : 1.7396096241875336, testing_acc : 0.7716346153846154\n",
            "training_loss : 0.2769997674557898, training_acc : 0.8972853535353534,testing_loss : 1.4146011976095347, testing_acc : 0.7740384615384616\n",
            "training_loss : 0.2873105294174618, training_acc : 0.8974431818181817,testing_loss : 1.5820652043017058, testing_acc : 0.7716346153846154\n",
            "training_loss : 0.29147215249637765, training_acc : 0.8949810606060606,testing_loss : 1.9214215010691147, testing_acc : 0.7620192307692307\n",
            "training_loss : 0.3073870799607701, training_acc : 0.88510101010101,testing_loss : 1.5822122796940117, testing_acc : 0.7764423076923077\n",
            "training_loss : 0.2899502408173349, training_acc : 0.9006944444444445,testing_loss : 1.4589714415084858, testing_acc : 0.7754807692307693\n",
            "training_loss : 0.2874093874461121, training_acc : 0.8969381313131313,testing_loss : 1.561303147043173, testing_acc : 0.7442307692307693\n",
            "training_loss : 0.2895719887067874, training_acc : 0.8965593434343434,testing_loss : 1.8268149969382927, testing_acc : 0.7548076923076923\n",
            "training_loss : 0.27460111843215096, training_acc : 0.8953914141414141,testing_loss : 1.8347132932752943, testing_acc : 0.7370192307692308\n"
          ]
        }
      ],
      "source": [
        "!python out_model_set/final_train.py --NUM_EPOCHS 130 --hidden_unit1 256 --hidden_unit2  108 --hidden_unit3 64 --hidden_unit4 64  --hidden_unit5 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfL_FqHQame_"
      },
      "outputs": [],
      "source": [
        "!python out_model_set/final_pred.py  --hidden_unit1 256 --hidden_unit2  108 --hidden_unit3 64 --hidden_unit4 64  --hidden_unit5 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsRdvAfnEkqe"
      },
      "outputs": [],
      "source": [
        "!python out_model_set/final_pred.py  --imagepath \"Testing/pituitary_tumor/image(54).jpg\" --hidden_unit1 256 --hidden_unit2  108 --hidden_unit3 64 --hidden_unit4 64  --hidden_unit5 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACLYcEmdUewT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torchvision\n",
        "\n",
        "print (torchvision.__version__,torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5zNIR_f-g51"
      },
      "outputs": [],
      "source": [
        "#from out_model_set import data_creator,model_creator,train_creator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMwI0qq1Chtk"
      },
      "outputs": [],
      "source": [
        "#data_creator.customdataset_loader()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}